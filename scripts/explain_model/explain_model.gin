from __gin__ import dynamic_registration
import __main__ as train_script
import sbf_modeling
import sbf_modeling.utils.data as data_utils
import transformers

# MODE is either "tests" or "deployment"
MODE="tests"
MODEL_DIR = %gin.required

transformers.Seq2SeqTrainingArguments:
    output_dir=".log/_explain_model"
    per_device_train_batch_size=2
    per_device_eval_batch_size=2
    evaluation_strategy="steps"
    eval_steps=100
    logging_steps=100
    gradient_accumulation_steps=8
    num_train_epochs=1
    weight_decay=0.1
    lr_scheduler_type="cosine"
    learning_rate=1e-4
    save_steps=5_000
    generation_max_length=512
    predict_with_generate=True  # generation in evaluation
    prediction_loss_only=False  # generation in evaluation

sbf_modeling.ExplainModel:
    t5_model_name = "google/flan-t5-small"

sbf_modeling.ExplainModel.train:
    args = @transformers.Seq2SeqTrainingArguments()
    print_prediction_num_examples = 300

data_utils.get_data:
    mode = %MODE

train_script.train:
    model = @sbf_modeling.ExplainModel()
    train_data = @data_utils.get_data()
    model_dir = %MODEL_DIR
